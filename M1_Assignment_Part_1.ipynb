{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Module-1-Assignment/blob/main/M1_Assignment_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "URHO-Uji2koG"
      },
      "source": [
        "# M1 Assignment  - Part 1: Text Processing and Edit Distance \n",
        "\n",
        "In this section we will be exploring how to preprocess tweets . We will provide a function for preprocessing tweets during this week's assignment, but it is still good to know what is going on under the hood. By the end of this assignment, you will see how to use the [NLTK](http://www.nltk.org) package to perform a preprocessing pipeline for Twitter datasets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cVg-26AV2koP"
      },
      "source": [
        "## Setup\n",
        "Eventually, you will conduct a sentiment analysis on Tweets. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. In this library you will use the NLTK to assist with processing the tweet to clean it for interpretation. \n",
        "\n",
        "In Part 1.1, you will call the Twitter API to return a series of tweets given an input, 'Elon Musk'. Next in Part 1.2, you will process the tweets using the various processing tasks outlined in the section and Chapter 2 of Jurafsky and Martin. Finally, in Part 1.3, you will create a simple version of a Levensthein distance formula to run the edit distance between two matrices. \n",
        "\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input. \n",
        "\n",
        "It will be look like following: \n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "td35EC362koR"
      },
      "source": [
        "## Part 1.1: Using the Twitter API to return Tweets\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IeRIfihz2koS"
      },
      "source": [
        "### Calling the Twitter API\n",
        "In this section, we will use the requests library to retrieve Tweets. Note that you will need to store these _twitter_keys.py_. in your specified library path to complete the portion of the assignment. The following code allows you to authenticate to teh Twitter API. You can run this code using the provided keys, to see that your expected output -- response code 200 -- results in a connection to the API. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PGyqiLrr2koT",
        "outputId": "97aa5bea-ba2b-4d73-91d2-22121dfa8253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WT2ZgK9mYqOYWFuvYxwrwSBBA\n",
            "None\n",
            "Response Code:  403\n"
          ]
        }
      ],
      "source": [
        "\"\"\"pip install requests\n",
        "pip install requests-oauthlib\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import sys\n",
        "sys.path.insert(0, \"./libs/\")\n",
        "import twitter_keys\n",
        "#this is a custom reference module to a package containing twitter keys\n",
        "#you will need to import this into your lib path to implement the following code\n",
        "\n",
        "\n",
        "%config IPCompleter.greedy=True\n",
        "key_secret = '{}:{}'.format(twitter_keys.client_key, twitter_keys.client_secret).encode('ascii')\n",
        "b64_encoded_key = base64.b64encode(key_secret)\n",
        "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
        "print(twitter_keys.client_key)\n",
        "\n",
        "    \n",
        "\n",
        "#identify base url and oauth token path\n",
        "base_url = 'https://api.twitter.com/' #base url for authentication\n",
        "auth_url = '{}oauth2/token'.format(base_url)\n",
        "#auth_url = 'https://api.twitter.com/1.1/account/verify_credentials.json'\n",
        "#share header information -- encoding is ascii\n",
        "auth_headers = {\n",
        "    'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
        "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
        "}\n",
        "\n",
        "\n",
        "#pass clientcredentials\n",
        "auth_data = {\n",
        "    'grant_type': 'client_credentials'\n",
        "}\n",
        "\n",
        "#send authentication using requests - POST request\n",
        "auth_resp = requests.post(auth_url, headers=auth_headers,auth=(twitter_keys.client_key, twitter_keys.client_secret), data = auth_data)\n",
        "#auth_resp = requests.post(auth_url,  auth=(twitter_keys.client_key, twitter_keys.client_secret), data = auth_data)\n",
        "\n",
        "bearer_token = auth_resp.json().get('access_token')\n",
        "print(bearer_token)\n",
        "\n",
        "#check response status. 200 = OK\n",
        "print(\"Response Code: \", auth_resp.status_code)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kdZaE1o42koX"
      },
      "source": [
        "Expected output should be: \n",
        "\n",
        "```\n",
        "Response Code:  200\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hzNDoS1P2koY"
      },
      "source": [
        "### Extracting tweets\n",
        "In this next section, you will submit a request to Twitter to retreive tweets. In this case, we are extracting the 10 most popular tweets with the text 'Elon Musk'.\n",
        "\n",
        "The full Twitter API Documentation can be found [here](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG9y6mEF2koZ",
        "outputId": "ebc4471b-328c-4a73-cd93-ee54e5b1aea6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'auth_resp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-aaea8e1622cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Keys in data response are token_type (bearer) and access_token (your access token)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccess_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauth_resp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'access_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m search_headers = {\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'Authorization'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Bearer {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'auth_resp' is not defined"
          ]
        }
      ],
      "source": [
        "#Keys in data response are token_type (bearer) and access_token (your access token)\n",
        "access_token = auth_resp.json()['access_token']\n",
        "\n",
        "search_headers = {\n",
        "    'Authorization': 'Bearer {}'.format(access_token)    \n",
        "}\n",
        "\n",
        "#enter search parameters in JSON string:\n",
        "query = 'Elon Musk'\n",
        "result_type = 'popular'\n",
        "count = 10\n",
        "lang = 'en'\n",
        "\n",
        "query_params = {\n",
        "    'q': query, #inputs the query parameter to filter tweets. \n",
        "    'result_type': result_type, #filters by most popular tweets\n",
        "    'count': count, #provides the top 15 results, defaults to 15\n",
        "    'lang': 'en' #filters by english language only\n",
        "}\n",
        "\n",
        "#identify search url path and save \n",
        "search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
        "#run search using get request\n",
        "search_resp = requests.get(search_url, headers=search_headers, params=query_params)\n",
        "\n",
        "#send request and print results \n",
        "print( count, \"tweets returned: \")\n",
        "twitter_data = search_resp.json()\n",
        "for x in twitter_data['statuses']:\n",
        "    print(x['text'] + '\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S2kk1YB-2koa"
      },
      "source": [
        "## Part 1.2: Preprocessing the text from tweets\n",
        "Text processing is one of the critical steps in an NLP project and in data scenience and analytics. It includes cleaning and formatting the data before feeding an algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
        "\n",
        "1. Tokenizing the string\n",
        "2. Lowercasing\n",
        "3. Removing stop words and punctuation\n",
        "4. Stemming\n",
        "\n",
        "We will take this approach with a selected tweet that we returned from above see how this is transformed by each preprocessing step.\n",
        "\n",
        "Let's take one of the tweets and apply preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W308paKm2kob",
        "outputId": "928ecb56-7cca-4948-a8c5-c023405da23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is how Elon Musk's mother raised successful children https://t.co/d7tfWPsCQs\n"
          ]
        }
      ],
      "source": [
        "#We're taking the 5th tweet parsed from above. It is complex enough that we can apply our preprocessing steps. \n",
        "tweet = twitter_data['statuses'][5]['text']\n",
        "print(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZoSOGaW2koc"
      },
      "outputs": [],
      "source": [
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgaZcBmu2koc",
        "outputId": "e0b2a8ac-415a-4b63-81fb-8cedf68cb0e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the stopwords from NLTK\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ii7mmr2kod"
      },
      "source": [
        "### Remove hyperlinks\n",
        "First, we will use regex to remove hyperlinks. You will create the regex substring to remove hyperlinks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQljVSFM2kod",
        "outputId": "d745bb8b-b10d-4d1e-991b-75dbdd3c798e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is how Elon Musk's mother raised successful children https://t.co/d7tfWPsCQs\n",
            "This is how Elon Musk's mother raised successful children \n"
          ]
        }
      ],
      "source": [
        "#reproduce the tweet we selected below: \n",
        "print(tweet)\n",
        "\n",
        "# remove old style retweet text \"RT\"\n",
        "tweet2 = re.sub(r'^RT[\\s]+', '', tweet) #run this to clean up old tweets\n",
        "\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "regex_remove_hyperlinks = None\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "# remove hyperlinks\n",
        "tweet2 = re.sub(regex_remove_hyperlinks, '', tweet2)\n",
        "\n",
        "# remove hashtags\n",
        "# only removing the hash # sign from the word\n",
        "tweet2 = re.sub(r'#', '', tweet2)\n",
        "\n",
        "print(tweet2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBLZmjp2koe"
      },
      "source": [
        "### Tokenize the string\n",
        "\n",
        "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD2ub7Dl2koe",
        "outputId": "9c5e0ec6-20e2-4e12-bece-00ee0e5ecc02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[92mThis is how Elon Musk's mother raised successful children \n",
            "\u001b[94m\n",
            "\n",
            "Tokenized string:\n",
            "['this', 'is', 'how', 'elon', \"musk's\", 'mother', 'raised', 'successful', 'children']\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print('\\033[92m' + tweet2)\n",
        "print('\\033[94m')\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "\n",
        "print()\n",
        "print('Tokenized string:')\n",
        "print(tweet_tokens)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8jxMlK2kof"
      },
      "source": [
        "### Remove stop words and punctuations\n",
        "\n",
        "The next step is to remove stop words and miscelleneous punctuations. Stop words are words that do not have semantic meaning to the tweet. There is a library of stopwords built into NLTK. The list provided by NLTK when you run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weMOqHRp2kof",
        "outputId": "19c59ebf-9118-4f2b-aa6b-64ab8d28c564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "#Import the english stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english') \n",
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "23rvPhYz2kof"
      },
      "source": [
        "We can see that the stop words list above contains some words that could be important in some contexts. \n",
        "These could be words like _i, not, between, because, won, against_. In some cases, you may want to update this dictionary of stop words to suit your needs. \n",
        "\n",
        "Certain groupings like ':)' and '...'  should be retained when dealing with tweets because they are used to express emotions, but in some cases they should be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jnynu5K2kog",
        "outputId": "aa7109f7-2ec5-46ca-8401-213bbd34d35b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'how', 'elon', \"musk's\", 'mother', 'raised', 'successful', 'children']\n",
            "\n",
            "removed stop words and punctuation:\n",
            "['elon', \"musk's\", 'mother', 'raised', 'successful', 'children']\n"
          ]
        }
      ],
      "source": [
        "print(tweet_tokens)\n",
        "print()\n",
        "\n",
        "tweets_clean = []\n",
        "\n",
        "for word in tweet_tokens: # Go through every word in your tokens list\n",
        "    if (word not in stopwords_english and  # remove stopwords\n",
        "        word not in string.punctuation):  # remove punctuation\n",
        "        tweets_clean.append(word)\n",
        "\n",
        "print('removed stop words and punctuation:')\n",
        "print(tweets_clean)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9quRejfi2kog"
      },
      "source": [
        "## preprocess_tweet()\n",
        "\n",
        "As shown above, preprocessing consists of multiple steps before you arrive at the final list of words.  In the week's assignment, you will use the function `preprocess_tweet(tweet)` available in _utils.py_. We encourage you to open the file and you'll see that this function's implementation is very similar to the steps above.\n",
        "\n",
        "To obtain the same result as in the previous code cells, you will only need to call the function `process_tweet()`. Let's do that in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frUPXf542koh"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "            \n",
        "    return tweets_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjioQEB12koh",
        "outputId": "d51f755d-83d4-4730-acb0-308485ad3ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "If Elon Musk gets his way, the people of Texas may soon count Tesla as one of its many electric utility options,\" s… https://t.co/KiD6rAFxeE\n",
            "preprocessed tweet:\n",
            "['elon', 'musk', 'get', 'way', 'peopl', 'texa', 'may', 'soon', 'count', 'tesla', 'one', 'mani', 'electr', 'util', 'option', '…']\n"
          ]
        }
      ],
      "source": [
        "#from utils import process_tweet # Import the process_tweet function\n",
        "\n",
        "# choose the same tweet\n",
        "tweet = twitter_data['statuses'][5]['text']\n",
        "\n",
        "print()\n",
        "print(tweet)\n",
        "\n",
        "\n",
        "# call the imported function\n",
        "tweets_stem = preprocess_tweet(tweet); # Preprocess a given tweet\n",
        "\n",
        "print('preprocessed tweet:')\n",
        "print(tweets_stem) # Print the result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MOFlGR0y2koi"
      },
      "source": [
        "## Part 1.3 Create a Levensthein Distance Formula"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "km5cU90S2koj"
      },
      "source": [
        "Recall that Edit Distance is the similarity between two words represented numericallly. Levenstehin distance is one of the most common algorithms used in calculating the edit distance between two words. \n",
        "\n",
        "Create your own simple Levensthein distance function. Then return the results of the distance between two words: _stemming_ and _lemmatization_.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh5Exy8R2koj",
        "outputId": "7541fad8-43d7-416e-d743-f9f4a7aa266d"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-92-d3c39890c585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mstring1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'stemming'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mstring2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lemmatization'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Your Levensthein Distance is: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleven_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstring2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-92-d3c39890c585>\u001b[0m in \u001b[0;36mleven_dist\u001b[1;34m(string1, string2)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#part I. calculate the numerical position of letter i-1, j and add 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mleven_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-92-d3c39890c585>\u001b[0m in \u001b[0;36mleven_dist\u001b[1;34m(string1, string2)\u001b[0m\n\u001b[0;32m      9\u001b[0m     '''\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#if min(i,j) =/= 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstring1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstring2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "def leven_dist(string1, string2):\n",
        "    '''\n",
        "    input: \n",
        "        string1 = the first word in your formula\n",
        "        string2 = the second word in your formula\n",
        "    output: \n",
        "        levenschtein edit distance\n",
        "    \n",
        "    '''\n",
        "    #if min(i,j) =/= 0 \n",
        "    if not string1: return len(string2)\n",
        "    if not string2: return len(string1)\n",
        "    \n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    #because min(i,j) =/= 0 then we min(i,j)\n",
        "    \n",
        "    return min(\n",
        "        \n",
        "        #part I. calculate the numerical position of letter i-1, j and add 1\n",
        "        leven_dist(None, None)+1, \n",
        "        \n",
        "\n",
        "        #part II: calculate the numerical position of letter i, j-1 and add 1\n",
        "        leven_dist(None, None)+1,\n",
        "        \n",
        "        # part III: if position i-1, j-1 are not the same letter, then add 1\n",
        "        leven_dist(None)+None\n",
        "    )\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "#now run your results\n",
        "\n",
        "string1 = 'stemming'\n",
        "string2 = 'lemmatization'\n",
        "print(\"Your Levensthein Distance is: \",leven_dist(string1,string2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5gh6B-2kok"
      },
      "source": [
        "### Expected output:\n",
        "Your Levensthein Distance is: 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J3PHkeRY2kol"
      },
      "source": [
        "#end of assignment#\n",
        "                                            \n",
        "Source: \n",
        "Natural Language preprocessing, deeplearning.ai\n",
        "Twitter API documentation\n",
        "Wikipedia: Levensthein Distance\n",
        "Chapter 2 (Jurafsky and Martin)\n",
        "                        \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
